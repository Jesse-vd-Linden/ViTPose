apex is not installed
apex is not installed
apex is not installed
load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/ssd/ssdlite_mobilenetv2_scratch_600e_coco/ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth
load checkpoint from local path: pytorch-checkpoint-models/vitpose+_base_whole.pth
The model and loaded state dict do not match exactly

size mismatch for backbone.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for backbone.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([576, 3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
size mismatch for backbone.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([768]).
size mismatch for keypoint_head.final_layer.weight: copying a param with shape torch.Size([17, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([133, 256, 1, 1]).
size mismatch for keypoint_head.final_layer.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([133]).
unexpected key in source state_dict: associate_keypoint_heads.0.deconv_layers.0.weight, associate_keypoint_heads.0.deconv_layers.1.weight, associate_keypoint_heads.0.deconv_layers.1.bias, associate_keypoint_heads.0.deconv_layers.1.running_mean, associate_keypoint_heads.0.deconv_layers.1.running_var, associate_keypoint_heads.0.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.0.deconv_layers.3.weight, associate_keypoint_heads.0.deconv_layers.4.weight, associate_keypoint_heads.0.deconv_layers.4.bias, associate_keypoint_heads.0.deconv_layers.4.running_mean, associate_keypoint_heads.0.deconv_layers.4.running_var, associate_keypoint_heads.0.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.0.final_layer.weight, associate_keypoint_heads.0.final_layer.bias, associate_keypoint_heads.1.deconv_layers.0.weight, associate_keypoint_heads.1.deconv_layers.1.weight, associate_keypoint_heads.1.deconv_layers.1.bias, associate_keypoint_heads.1.deconv_layers.1.running_mean, associate_keypoint_heads.1.deconv_layers.1.running_var, associate_keypoint_heads.1.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.1.deconv_layers.3.weight, associate_keypoint_heads.1.deconv_layers.4.weight, associate_keypoint_heads.1.deconv_layers.4.bias, associate_keypoint_heads.1.deconv_layers.4.running_mean, associate_keypoint_heads.1.deconv_layers.4.running_var, associate_keypoint_heads.1.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.1.final_layer.weight, associate_keypoint_heads.1.final_layer.bias, associate_keypoint_heads.2.deconv_layers.0.weight, associate_keypoint_heads.2.deconv_layers.1.weight, associate_keypoint_heads.2.deconv_layers.1.bias, associate_keypoint_heads.2.deconv_layers.1.running_mean, associate_keypoint_heads.2.deconv_layers.1.running_var, associate_keypoint_heads.2.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.2.deconv_layers.3.weight, associate_keypoint_heads.2.deconv_layers.4.weight, associate_keypoint_heads.2.deconv_layers.4.bias, associate_keypoint_heads.2.deconv_layers.4.running_mean, associate_keypoint_heads.2.deconv_layers.4.running_var, associate_keypoint_heads.2.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.2.final_layer.weight, associate_keypoint_heads.2.final_layer.bias, associate_keypoint_heads.3.deconv_layers.0.weight, associate_keypoint_heads.3.deconv_layers.1.weight, associate_keypoint_heads.3.deconv_layers.1.bias, associate_keypoint_heads.3.deconv_layers.1.running_mean, associate_keypoint_heads.3.deconv_layers.1.running_var, associate_keypoint_heads.3.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.3.deconv_layers.3.weight, associate_keypoint_heads.3.deconv_layers.4.weight, associate_keypoint_heads.3.deconv_layers.4.bias, associate_keypoint_heads.3.deconv_layers.4.running_mean, associate_keypoint_heads.3.deconv_layers.4.running_var, associate_keypoint_heads.3.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.3.final_layer.weight, associate_keypoint_heads.3.final_layer.bias, associate_keypoint_heads.4.deconv_layers.0.weight, associate_keypoint_heads.4.deconv_layers.1.weight, associate_keypoint_heads.4.deconv_layers.1.bias, associate_keypoint_heads.4.deconv_layers.1.running_mean, associate_keypoint_heads.4.deconv_layers.1.running_var, associate_keypoint_heads.4.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.4.deconv_layers.3.weight, associate_keypoint_heads.4.deconv_layers.4.weight, associate_keypoint_heads.4.deconv_layers.4.bias, associate_keypoint_heads.4.deconv_layers.4.running_mean, associate_keypoint_heads.4.deconv_layers.4.running_var, associate_keypoint_heads.4.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.4.final_layer.weight, associate_keypoint_heads.4.final_layer.bias, backbone.blocks.0.mlp.experts.0.weight, backbone.blocks.0.mlp.experts.0.bias, backbone.blocks.0.mlp.experts.1.weight, backbone.blocks.0.mlp.experts.1.bias, backbone.blocks.0.mlp.experts.2.weight, backbone.blocks.0.mlp.experts.2.bias, backbone.blocks.0.mlp.experts.3.weight, backbone.blocks.0.mlp.experts.3.bias, backbone.blocks.0.mlp.experts.4.weight, backbone.blocks.0.mlp.experts.4.bias, backbone.blocks.0.mlp.experts.5.weight, backbone.blocks.0.mlp.experts.5.bias, backbone.blocks.1.mlp.experts.0.weight, backbone.blocks.1.mlp.experts.0.bias, backbone.blocks.1.mlp.experts.1.weight, backbone.blocks.1.mlp.experts.1.bias, backbone.blocks.1.mlp.experts.2.weight, backbone.blocks.1.mlp.experts.2.bias, backbone.blocks.1.mlp.experts.3.weight, backbone.blocks.1.mlp.experts.3.bias, backbone.blocks.1.mlp.experts.4.weight, backbone.blocks.1.mlp.experts.4.bias, backbone.blocks.1.mlp.experts.5.weight, backbone.blocks.1.mlp.experts.5.bias, backbone.blocks.2.mlp.experts.0.weight, backbone.blocks.2.mlp.experts.0.bias, backbone.blocks.2.mlp.experts.1.weight, backbone.blocks.2.mlp.experts.1.bias, backbone.blocks.2.mlp.experts.2.weight, backbone.blocks.2.mlp.experts.2.bias, backbone.blocks.2.mlp.experts.3.weight, backbone.blocks.2.mlp.experts.3.bias, backbone.blocks.2.mlp.experts.4.weight, backbone.blocks.2.mlp.experts.4.bias, backbone.blocks.2.mlp.experts.5.weight, backbone.blocks.2.mlp.experts.5.bias, backbone.blocks.3.mlp.experts.0.weight, backbone.blocks.3.mlp.experts.0.bias, backbone.blocks.3.mlp.experts.1.weight, backbone.blocks.3.mlp.experts.1.bias, backbone.blocks.3.mlp.experts.2.weight, backbone.blocks.3.mlp.experts.2.bias, backbone.blocks.3.mlp.experts.3.weight, backbone.blocks.3.mlp.experts.3.bias, backbone.blocks.3.mlp.experts.4.weight, backbone.blocks.3.mlp.experts.4.bias, backbone.blocks.3.mlp.experts.5.weight, backbone.blocks.3.mlp.experts.5.bias, backbone.blocks.4.mlp.experts.0.weight, backbone.blocks.4.mlp.experts.0.bias, backbone.blocks.4.mlp.experts.1.weight, backbone.blocks.4.mlp.experts.1.bias, backbone.blocks.4.mlp.experts.2.weight, backbone.blocks.4.mlp.experts.2.bias, backbone.blocks.4.mlp.experts.3.weight, backbone.blocks.4.mlp.experts.3.bias, backbone.blocks.4.mlp.experts.4.weight, backbone.blocks.4.mlp.experts.4.bias, backbone.blocks.4.mlp.experts.5.weight, backbone.blocks.4.mlp.experts.5.bias, backbone.blocks.5.mlp.experts.0.weight, backbone.blocks.5.mlp.experts.0.bias, backbone.blocks.5.mlp.experts.1.weight, backbone.blocks.5.mlp.experts.1.bias, backbone.blocks.5.mlp.experts.2.weight, backbone.blocks.5.mlp.experts.2.bias, backbone.blocks.5.mlp.experts.3.weight, backbone.blocks.5.mlp.experts.3.bias, backbone.blocks.5.mlp.experts.4.weight, backbone.blocks.5.mlp.experts.4.bias, backbone.blocks.5.mlp.experts.5.weight, backbone.blocks.5.mlp.experts.5.bias, backbone.blocks.6.mlp.experts.0.weight, backbone.blocks.6.mlp.experts.0.bias, backbone.blocks.6.mlp.experts.1.weight, backbone.blocks.6.mlp.experts.1.bias, backbone.blocks.6.mlp.experts.2.weight, backbone.blocks.6.mlp.experts.2.bias, backbone.blocks.6.mlp.experts.3.weight, backbone.blocks.6.mlp.experts.3.bias, backbone.blocks.6.mlp.experts.4.weight, backbone.blocks.6.mlp.experts.4.bias, backbone.blocks.6.mlp.experts.5.weight, backbone.blocks.6.mlp.experts.5.bias, backbone.blocks.7.mlp.experts.0.weight, backbone.blocks.7.mlp.experts.0.bias, backbone.blocks.7.mlp.experts.1.weight, backbone.blocks.7.mlp.experts.1.bias, backbone.blocks.7.mlp.experts.2.weight, backbone.blocks.7.mlp.experts.2.bias, backbone.blocks.7.mlp.experts.3.weight, backbone.blocks.7.mlp.experts.3.bias, backbone.blocks.7.mlp.experts.4.weight, backbone.blocks.7.mlp.experts.4.bias, backbone.blocks.7.mlp.experts.5.weight, backbone.blocks.7.mlp.experts.5.bias, backbone.blocks.8.mlp.experts.0.weight, backbone.blocks.8.mlp.experts.0.bias, backbone.blocks.8.mlp.experts.1.weight, backbone.blocks.8.mlp.experts.1.bias, backbone.blocks.8.mlp.experts.2.weight, backbone.blocks.8.mlp.experts.2.bias, backbone.blocks.8.mlp.experts.3.weight, backbone.blocks.8.mlp.experts.3.bias, backbone.blocks.8.mlp.experts.4.weight, backbone.blocks.8.mlp.experts.4.bias, backbone.blocks.8.mlp.experts.5.weight, backbone.blocks.8.mlp.experts.5.bias, backbone.blocks.9.mlp.experts.0.weight, backbone.blocks.9.mlp.experts.0.bias, backbone.blocks.9.mlp.experts.1.weight, backbone.blocks.9.mlp.experts.1.bias, backbone.blocks.9.mlp.experts.2.weight, backbone.blocks.9.mlp.experts.2.bias, backbone.blocks.9.mlp.experts.3.weight, backbone.blocks.9.mlp.experts.3.bias, backbone.blocks.9.mlp.experts.4.weight, backbone.blocks.9.mlp.experts.4.bias, backbone.blocks.9.mlp.experts.5.weight, backbone.blocks.9.mlp.experts.5.bias, backbone.blocks.10.mlp.experts.0.weight, backbone.blocks.10.mlp.experts.0.bias, backbone.blocks.10.mlp.experts.1.weight, backbone.blocks.10.mlp.experts.1.bias, backbone.blocks.10.mlp.experts.2.weight, backbone.blocks.10.mlp.experts.2.bias, backbone.blocks.10.mlp.experts.3.weight, backbone.blocks.10.mlp.experts.3.bias, backbone.blocks.10.mlp.experts.4.weight, backbone.blocks.10.mlp.experts.4.bias, backbone.blocks.10.mlp.experts.5.weight, backbone.blocks.10.mlp.experts.5.bias, backbone.blocks.11.mlp.experts.0.weight, backbone.blocks.11.mlp.experts.0.bias, backbone.blocks.11.mlp.experts.1.weight, backbone.blocks.11.mlp.experts.1.bias, backbone.blocks.11.mlp.experts.2.weight, backbone.blocks.11.mlp.experts.2.bias, backbone.blocks.11.mlp.experts.3.weight, backbone.blocks.11.mlp.experts.3.bias, backbone.blocks.11.mlp.experts.4.weight, backbone.blocks.11.mlp.experts.4.bias, backbone.blocks.11.mlp.experts.5.weight, backbone.blocks.11.mlp.experts.5.bias

Thread "input" started
Thread "det" started
Thread "pose" started
Inference data
{'vitpose-b-wholebody': {'inference count': 319, 'inference times': [0.5774805545806885, 0.13388562202453613, 0.13482952117919922, 0.13739013671875, 0.13898253440856934, 0.13671088218688965, 0.13666534423828125, 0.13731956481933594, 0.13928937911987305, 0.13498163223266602, 0.13590383529663086, 0.13582682609558105, 0.13791775703430176, 0.14037728309631348, 0.14127588272094727, 0.13913512229919434, 0.1356980800628662, 0.1370069980621338, 0.13753890991210938, 0.13861942291259766, 0.1384272575378418, 0.14564776420593262, 0.13941121101379395, 0.13742518424987793, 0.14705348014831543, 0.13693785667419434, 0.13733458518981934, 0.13987946510314941, 0.1377551555633545, 0.13831090927124023, 0.13651061058044434, 0.15143585205078125, 0.13848543167114258, 0.1338803768157959, 0.13891887664794922, 0.1355140209197998, 3.0964832305908203, 2.0340607166290283, 0.1458282470703125, 0.12253999710083008, 0.13869643211364746, 0.1341092586517334, 0.1371288299560547, 0.13837623596191406, 0.13829421997070312, 0.13964319229125977, 0.13919520378112793, 0.13495349884033203, 0.13682961463928223, 0.1398477554321289, 0.13963532447814941, 0.13892793655395508, 0.1392686367034912, 0.1386103630065918, 0.13708758354187012, 0.1389317512512207, 0.13871455192565918, 0.13643741607666016, 0.13831496238708496, 0.13672089576721191, 0.1343703269958496, 0.1387624740600586, 0.1339116096496582, 0.13783812522888184, 0.13673019409179688, 0.13844776153564453, 0.1440415382385254, 0.13684988021850586, 0.14112401008605957, 0.13982820510864258, 0.13619256019592285, 0.13837456703186035, 0.14284825325012207, 0.1354682445526123, 3.691220760345459, 3.701267719268799, 1.7396934032440186, 0.14382147789001465, 0.14241743087768555, 0.14180397987365723, 0.14285755157470703, 0.14250993728637695, 0.14136815071105957, 0.14196419715881348, 0.1405353546142578, 0.143052339553833, 0.14128637313842773, 0.1436293125152588, 0.13867783546447754, 0.14255166053771973, 0.140977144241333, 0.14175105094909668, 0.14243483543395996, 0.14432668685913086, 0.14380598068237305, 0.14120793342590332, 0.14096522331237793, 0.13786077499389648, 0.14139890670776367, 0.14046525955200195, 0.14232659339904785, 0.14139032363891602, 0.1422443389892578, 0.14314913749694824, 0.14433932304382324, 0.1398930549621582, 0.14265012741088867, 0.13979434967041016, 0.13835763931274414, 0.14208364486694336, 0.1442406177520752, 0.13923001289367676, 0.13986563682556152, 0.1267235279083252, 0.1359407901763916, 0.14159274101257324, 0.13950252532958984, 0.14014625549316406, 0.13804221153259277, 0.14278483390808105, 0.14174628257751465, 0.14445257186889648, 0.1410984992980957, 0.1423778533935547, 0.14266228675842285, 0.1418771743774414, 0.14197301864624023, 0.13984417915344238, 0.14213323593139648, 0.1441495418548584, 0.14307737350463867, 0.13998055458068848, 0.14117074012756348, 0.1423664093017578, 0.1398003101348877, 0.14564967155456543, 0.14327120780944824, 0.14232134819030762, 0.1258375644683838, 0.13837957382202148, 0.14156174659729004, 0.14189410209655762, 0.14145565032958984, 0.14044952392578125, 0.14039897918701172, 0.14237093925476074, 0.1431560516357422, 0.14193177223205566, 0.1415557861328125, 0.14343023300170898, 0.14394521713256836, 0.1378946304321289, 0.1399383544921875, 0.1405656337738037, 0.15428376197814941, 0.14015460014343262, 0.14531874656677246, 0.14252734184265137, 0.14471960067749023, 0.14114141464233398, 0.14360332489013672, 0.13999629020690918, 0.1435565948486328, 0.14255237579345703, 0.13808894157409668, 0.1385664939880371, 0.14153194427490234, 0.15072941780090332, 0.14890074729919434, 0.16061997413635254, 0.1615283489227295, 0.16503024101257324, 0.16556477546691895, 0.1764688491821289, 0.16638898849487305, 0.1663980484008789, 0.15795302391052246, 0.14633917808532715, 0.1431574821472168, 0.14574861526489258, 0.14483165740966797, 0.14563894271850586, 0.14563584327697754, 0.14031052589416504, 0.15374350547790527, 0.15600848197937012, 0.18054604530334473, 0.19088172912597656, 0.2137606143951416, 0.18598556518554688, 0.17664337158203125, 0.1716148853302002, 0.1723313331604004, 0.17606091499328613, 0.14313220977783203, 0.14316892623901367, 0.14087295532226562, 0.1407480239868164, 0.1418776512145996, 0.1398005485534668, 0.13890647888183594, 0.14353585243225098, 0.14498686790466309, 0.14418911933898926, 0.13965916633605957, 0.14227771759033203, 0.13930320739746094, 0.14694499969482422, 0.14343547821044922, 0.14192461967468262, 0.1432511806488037, 0.14378070831298828, 0.14397692680358887, 0.14240121841430664, 0.12712907791137695, 0.14288926124572754, 0.14888215065002441, 0.14995527267456055, 0.14656805992126465, 0.14277935028076172, 0.14171624183654785, 0.13886451721191406, 0.1418464183807373, 0.14330410957336426, 0.147324800491333, 0.14524221420288086, 0.1448831558227539, 0.14526796340942383, 0.1481626033782959, 0.14161300659179688, 0.14178133010864258, 0.14264512062072754, 0.1425485610961914, 0.14539217948913574, 0.14363908767700195, 0.142899751663208, 0.1424417495727539, 0.14175033569335938, 0.14615154266357422, 0.14053726196289062, 0.14330816268920898, 0.14824771881103516, 0.14055180549621582, 0.1446990966796875, 0.14470481872558594, 0.14568305015563965, 0.14946556091308594, 0.14182472229003906, 0.13269996643066406, 0.14356780052185059, 0.1494126319885254, 0.1449270248413086, 0.14464569091796875, 0.14267301559448242, 0.14498138427734375, 0.1399681568145752, 0.14829778671264648, 0.14314937591552734, 0.14188814163208008, 0.14588642120361328, 0.1445937156677246, 0.1428055763244629, 0.14455652236938477, 0.14315176010131836, 0.14241480827331543, 0.1445770263671875, 0.14595365524291992, 0.14374685287475586, 0.14301514625549316, 0.14278554916381836, 0.14423441886901855, 0.14254450798034668, 0.1460733413696289, 0.1419820785522461, 0.14641070365905762, 0.15160822868347168, 0.14472699165344238, 0.14223313331604004, 0.14534759521484375, 0.16305041313171387, 0.1491985321044922, 0.14890456199645996, 0.15177321434020996, 0.16945838928222656, 0.16467785835266113, 0.17623686790466309, 0.1739816665649414, 0.16218256950378418, 0.15267634391784668, 0.14243721961975098, 0.14500689506530762, 0.1556258201599121, 0.1431291103363037, 0.14392805099487305, 0.142866849899292, 0.13954901695251465, 0.14003443717956543, 0.14379382133483887, 0.1451268196105957, 0.1425304412841797, 0.1426401138305664, 0.13975882530212402, 0.14714956283569336, 0.14322352409362793, 0.1413569450378418, 0.14174723625183105, 0.14392566680908203, 0.14012765884399414, 0.14121627807617188, 0.14686322212219238, 0.14182353019714355, 0.15894222259521484, 0.17278051376342773, 0.1737680435180664, 0.17313194274902344, 0.17006611824035645, 0.14874982833862305, 0.14470696449279785, 0.1476597785949707]}}
--------------
{'vitpose-b-wholebody': {'Average inference fps': 7.023244034439363,
                         'Average inference time': 0.14238434476950734,
                         'inference count': 319}}
