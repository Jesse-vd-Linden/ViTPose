apex is not installed
apex is not installed
apex is not installed
load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/ssd/ssdlite_mobilenetv2_scratch_600e_coco/ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth
load checkpoint from local path: pytorch-checkpoint-models/vitpose+_small_whole.pth
The model and loaded state dict do not match exactly

size mismatch for backbone.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for keypoint_head.final_layer.weight: copying a param with shape torch.Size([17, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([133, 256, 1, 1]).
size mismatch for keypoint_head.final_layer.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([133]).
unexpected key in source state_dict: associate_keypoint_heads.0.deconv_layers.0.weight, associate_keypoint_heads.0.deconv_layers.1.weight, associate_keypoint_heads.0.deconv_layers.1.bias, associate_keypoint_heads.0.deconv_layers.1.running_mean, associate_keypoint_heads.0.deconv_layers.1.running_var, associate_keypoint_heads.0.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.0.deconv_layers.3.weight, associate_keypoint_heads.0.deconv_layers.4.weight, associate_keypoint_heads.0.deconv_layers.4.bias, associate_keypoint_heads.0.deconv_layers.4.running_mean, associate_keypoint_heads.0.deconv_layers.4.running_var, associate_keypoint_heads.0.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.0.final_layer.weight, associate_keypoint_heads.0.final_layer.bias, associate_keypoint_heads.1.deconv_layers.0.weight, associate_keypoint_heads.1.deconv_layers.1.weight, associate_keypoint_heads.1.deconv_layers.1.bias, associate_keypoint_heads.1.deconv_layers.1.running_mean, associate_keypoint_heads.1.deconv_layers.1.running_var, associate_keypoint_heads.1.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.1.deconv_layers.3.weight, associate_keypoint_heads.1.deconv_layers.4.weight, associate_keypoint_heads.1.deconv_layers.4.bias, associate_keypoint_heads.1.deconv_layers.4.running_mean, associate_keypoint_heads.1.deconv_layers.4.running_var, associate_keypoint_heads.1.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.1.final_layer.weight, associate_keypoint_heads.1.final_layer.bias, associate_keypoint_heads.2.deconv_layers.0.weight, associate_keypoint_heads.2.deconv_layers.1.weight, associate_keypoint_heads.2.deconv_layers.1.bias, associate_keypoint_heads.2.deconv_layers.1.running_mean, associate_keypoint_heads.2.deconv_layers.1.running_var, associate_keypoint_heads.2.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.2.deconv_layers.3.weight, associate_keypoint_heads.2.deconv_layers.4.weight, associate_keypoint_heads.2.deconv_layers.4.bias, associate_keypoint_heads.2.deconv_layers.4.running_mean, associate_keypoint_heads.2.deconv_layers.4.running_var, associate_keypoint_heads.2.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.2.final_layer.weight, associate_keypoint_heads.2.final_layer.bias, associate_keypoint_heads.3.deconv_layers.0.weight, associate_keypoint_heads.3.deconv_layers.1.weight, associate_keypoint_heads.3.deconv_layers.1.bias, associate_keypoint_heads.3.deconv_layers.1.running_mean, associate_keypoint_heads.3.deconv_layers.1.running_var, associate_keypoint_heads.3.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.3.deconv_layers.3.weight, associate_keypoint_heads.3.deconv_layers.4.weight, associate_keypoint_heads.3.deconv_layers.4.bias, associate_keypoint_heads.3.deconv_layers.4.running_mean, associate_keypoint_heads.3.deconv_layers.4.running_var, associate_keypoint_heads.3.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.3.final_layer.weight, associate_keypoint_heads.3.final_layer.bias, associate_keypoint_heads.4.deconv_layers.0.weight, associate_keypoint_heads.4.deconv_layers.1.weight, associate_keypoint_heads.4.deconv_layers.1.bias, associate_keypoint_heads.4.deconv_layers.1.running_mean, associate_keypoint_heads.4.deconv_layers.1.running_var, associate_keypoint_heads.4.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.4.deconv_layers.3.weight, associate_keypoint_heads.4.deconv_layers.4.weight, associate_keypoint_heads.4.deconv_layers.4.bias, associate_keypoint_heads.4.deconv_layers.4.running_mean, associate_keypoint_heads.4.deconv_layers.4.running_var, associate_keypoint_heads.4.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.4.final_layer.weight, associate_keypoint_heads.4.final_layer.bias, backbone.blocks.0.mlp.experts.0.weight, backbone.blocks.0.mlp.experts.0.bias, backbone.blocks.0.mlp.experts.1.weight, backbone.blocks.0.mlp.experts.1.bias, backbone.blocks.0.mlp.experts.2.weight, backbone.blocks.0.mlp.experts.2.bias, backbone.blocks.0.mlp.experts.3.weight, backbone.blocks.0.mlp.experts.3.bias, backbone.blocks.0.mlp.experts.4.weight, backbone.blocks.0.mlp.experts.4.bias, backbone.blocks.0.mlp.experts.5.weight, backbone.blocks.0.mlp.experts.5.bias, backbone.blocks.1.mlp.experts.0.weight, backbone.blocks.1.mlp.experts.0.bias, backbone.blocks.1.mlp.experts.1.weight, backbone.blocks.1.mlp.experts.1.bias, backbone.blocks.1.mlp.experts.2.weight, backbone.blocks.1.mlp.experts.2.bias, backbone.blocks.1.mlp.experts.3.weight, backbone.blocks.1.mlp.experts.3.bias, backbone.blocks.1.mlp.experts.4.weight, backbone.blocks.1.mlp.experts.4.bias, backbone.blocks.1.mlp.experts.5.weight, backbone.blocks.1.mlp.experts.5.bias, backbone.blocks.2.mlp.experts.0.weight, backbone.blocks.2.mlp.experts.0.bias, backbone.blocks.2.mlp.experts.1.weight, backbone.blocks.2.mlp.experts.1.bias, backbone.blocks.2.mlp.experts.2.weight, backbone.blocks.2.mlp.experts.2.bias, backbone.blocks.2.mlp.experts.3.weight, backbone.blocks.2.mlp.experts.3.bias, backbone.blocks.2.mlp.experts.4.weight, backbone.blocks.2.mlp.experts.4.bias, backbone.blocks.2.mlp.experts.5.weight, backbone.blocks.2.mlp.experts.5.bias, backbone.blocks.3.mlp.experts.0.weight, backbone.blocks.3.mlp.experts.0.bias, backbone.blocks.3.mlp.experts.1.weight, backbone.blocks.3.mlp.experts.1.bias, backbone.blocks.3.mlp.experts.2.weight, backbone.blocks.3.mlp.experts.2.bias, backbone.blocks.3.mlp.experts.3.weight, backbone.blocks.3.mlp.experts.3.bias, backbone.blocks.3.mlp.experts.4.weight, backbone.blocks.3.mlp.experts.4.bias, backbone.blocks.3.mlp.experts.5.weight, backbone.blocks.3.mlp.experts.5.bias, backbone.blocks.4.mlp.experts.0.weight, backbone.blocks.4.mlp.experts.0.bias, backbone.blocks.4.mlp.experts.1.weight, backbone.blocks.4.mlp.experts.1.bias, backbone.blocks.4.mlp.experts.2.weight, backbone.blocks.4.mlp.experts.2.bias, backbone.blocks.4.mlp.experts.3.weight, backbone.blocks.4.mlp.experts.3.bias, backbone.blocks.4.mlp.experts.4.weight, backbone.blocks.4.mlp.experts.4.bias, backbone.blocks.4.mlp.experts.5.weight, backbone.blocks.4.mlp.experts.5.bias, backbone.blocks.5.mlp.experts.0.weight, backbone.blocks.5.mlp.experts.0.bias, backbone.blocks.5.mlp.experts.1.weight, backbone.blocks.5.mlp.experts.1.bias, backbone.blocks.5.mlp.experts.2.weight, backbone.blocks.5.mlp.experts.2.bias, backbone.blocks.5.mlp.experts.3.weight, backbone.blocks.5.mlp.experts.3.bias, backbone.blocks.5.mlp.experts.4.weight, backbone.blocks.5.mlp.experts.4.bias, backbone.blocks.5.mlp.experts.5.weight, backbone.blocks.5.mlp.experts.5.bias, backbone.blocks.6.mlp.experts.0.weight, backbone.blocks.6.mlp.experts.0.bias, backbone.blocks.6.mlp.experts.1.weight, backbone.blocks.6.mlp.experts.1.bias, backbone.blocks.6.mlp.experts.2.weight, backbone.blocks.6.mlp.experts.2.bias, backbone.blocks.6.mlp.experts.3.weight, backbone.blocks.6.mlp.experts.3.bias, backbone.blocks.6.mlp.experts.4.weight, backbone.blocks.6.mlp.experts.4.bias, backbone.blocks.6.mlp.experts.5.weight, backbone.blocks.6.mlp.experts.5.bias, backbone.blocks.7.mlp.experts.0.weight, backbone.blocks.7.mlp.experts.0.bias, backbone.blocks.7.mlp.experts.1.weight, backbone.blocks.7.mlp.experts.1.bias, backbone.blocks.7.mlp.experts.2.weight, backbone.blocks.7.mlp.experts.2.bias, backbone.blocks.7.mlp.experts.3.weight, backbone.blocks.7.mlp.experts.3.bias, backbone.blocks.7.mlp.experts.4.weight, backbone.blocks.7.mlp.experts.4.bias, backbone.blocks.7.mlp.experts.5.weight, backbone.blocks.7.mlp.experts.5.bias, backbone.blocks.8.mlp.experts.0.weight, backbone.blocks.8.mlp.experts.0.bias, backbone.blocks.8.mlp.experts.1.weight, backbone.blocks.8.mlp.experts.1.bias, backbone.blocks.8.mlp.experts.2.weight, backbone.blocks.8.mlp.experts.2.bias, backbone.blocks.8.mlp.experts.3.weight, backbone.blocks.8.mlp.experts.3.bias, backbone.blocks.8.mlp.experts.4.weight, backbone.blocks.8.mlp.experts.4.bias, backbone.blocks.8.mlp.experts.5.weight, backbone.blocks.8.mlp.experts.5.bias, backbone.blocks.9.mlp.experts.0.weight, backbone.blocks.9.mlp.experts.0.bias, backbone.blocks.9.mlp.experts.1.weight, backbone.blocks.9.mlp.experts.1.bias, backbone.blocks.9.mlp.experts.2.weight, backbone.blocks.9.mlp.experts.2.bias, backbone.blocks.9.mlp.experts.3.weight, backbone.blocks.9.mlp.experts.3.bias, backbone.blocks.9.mlp.experts.4.weight, backbone.blocks.9.mlp.experts.4.bias, backbone.blocks.9.mlp.experts.5.weight, backbone.blocks.9.mlp.experts.5.bias, backbone.blocks.10.mlp.experts.0.weight, backbone.blocks.10.mlp.experts.0.bias, backbone.blocks.10.mlp.experts.1.weight, backbone.blocks.10.mlp.experts.1.bias, backbone.blocks.10.mlp.experts.2.weight, backbone.blocks.10.mlp.experts.2.bias, backbone.blocks.10.mlp.experts.3.weight, backbone.blocks.10.mlp.experts.3.bias, backbone.blocks.10.mlp.experts.4.weight, backbone.blocks.10.mlp.experts.4.bias, backbone.blocks.10.mlp.experts.5.weight, backbone.blocks.10.mlp.experts.5.bias, backbone.blocks.11.mlp.experts.0.weight, backbone.blocks.11.mlp.experts.0.bias, backbone.blocks.11.mlp.experts.1.weight, backbone.blocks.11.mlp.experts.1.bias, backbone.blocks.11.mlp.experts.2.weight, backbone.blocks.11.mlp.experts.2.bias, backbone.blocks.11.mlp.experts.3.weight, backbone.blocks.11.mlp.experts.3.bias, backbone.blocks.11.mlp.experts.4.weight, backbone.blocks.11.mlp.experts.4.bias, backbone.blocks.11.mlp.experts.5.weight, backbone.blocks.11.mlp.experts.5.bias

Thread "input" started
Thread "det" started
Thread "pose" started
False
Inference data
{'vitpose-s-wholebody': {'inference count': 445, 'inference times': [0.8187024593353271, 0.12001895904541016, 0.09992194175720215, 0.13147306442260742, 0.1327815055847168, 0.13118553161621094, 0.1422107219696045, 0.12639546394348145, 0.13305902481079102, 0.1327812671661377, 0.13732600212097168, 0.13318824768066406, 0.13138294219970703, 0.13193607330322266, 0.1323404312133789, 0.1371781826019287, 0.1266932487487793, 0.13736295700073242, 0.13232159614562988, 0.13790488243103027, 0.13254809379577637, 0.12987208366394043, 0.1358792781829834, 0.13497328758239746, 0.12862944602966309, 0.14024686813354492, 0.12961125373840332, 0.14024972915649414, 0.12958168983459473, 0.1356973648071289, 0.13363957405090332, 0.1314222812652588, 0.13155698776245117, 0.13506174087524414, 0.13022923469543457, 0.13572978973388672, 0.1423649787902832, 0.12173175811767578, 0.13549041748046875, 0.14756345748901367, 0.11982607841491699, 0.13205480575561523, 0.13062429428100586, 0.13945364952087402, 0.1314082145690918, 0.12964916229248047, 0.14126253128051758, 0.12919235229492188, 0.1334998607635498, 0.13691091537475586, 0.1299734115600586, 0.13749098777770996, 0.13156485557556152, 0.13245606422424316, 0.13683605194091797, 0.12888860702514648, 0.13662981986999512, 0.13129186630249023, 0.1347651481628418, 0.13407540321350098, 0.13350272178649902, 0.13052010536193848, 0.14130139350891113, 0.12548565864562988, 0.13197088241577148, 0.13876724243164062, 0.13080644607543945, 0.13657021522521973, 0.1312732696533203, 0.1306312084197998, 0.1365354061126709, 0.13565731048583984, 0.13169288635253906, 0.1325540542602539, 0.13583922386169434, 0.13410711288452148, 0.12990689277648926, 0.1351175308227539, 0.13389086723327637, 0.13045787811279297, 0.13448786735534668, 0.13418173789978027, 0.1316664218902588, 0.13707304000854492, 0.13419818878173828, 0.12860393524169922, 0.13689303398132324, 0.13014793395996094, 0.13520002365112305, 0.1383213996887207, 0.12944531440734863, 0.13529014587402344, 0.13376259803771973, 0.13022947311401367, 0.14687657356262207, 0.12184643745422363, 0.13962411880493164, 0.1296696662902832, 0.13275623321533203, 0.13887405395507812, 0.13366389274597168, 0.12923455238342285, 0.13869237899780273, 0.1417217254638672, 0.13005757331848145, 0.12914443016052246, 0.130964994430542, 0.13639092445373535, 0.1322956085205078, 0.1295912265777588, 0.13552021980285645, 0.13508272171020508, 0.1351003646850586, 0.13414645195007324, 0.13035321235656738, 0.1363389492034912, 0.1303846836090088, 0.13396954536437988, 0.13981342315673828, 0.1275641918182373, 0.13371801376342773, 0.13945674896240234, 0.12521958351135254, 0.13909459114074707, 0.12888145446777344, 0.13455438613891602, 0.1378774642944336, 0.12997746467590332, 0.13933539390563965, 0.12614178657531738, 0.13312005996704102, 0.13808274269104004, 0.1332998275756836, 0.13080072402954102, 0.130141019821167, 0.13581323623657227, 0.1354687213897705, 0.1332263946533203, 0.13249969482421875, 0.1355125904083252, 0.13094091415405273, 0.12865018844604492, 0.1395418643951416, 0.1312265396118164, 0.13684844970703125, 0.133134126663208, 0.1322646141052246, 0.13483834266662598, 0.13383746147155762, 0.1322629451751709, 0.13340210914611816, 0.13019657135009766, 0.1402597427368164, 0.1322023868560791, 0.1269547939300537, 0.13859891891479492, 0.13684344291687012, 0.12412858009338379, 0.14091181755065918, 0.12959527969360352, 0.13591980934143066, 0.13152265548706055, 0.13314175605773926, 0.13708233833312988, 0.13037633895874023, 0.13575148582458496, 0.13186049461364746, 0.13255596160888672, 0.13508391380310059, 0.13570022583007812, 0.13412213325500488, 0.13019180297851562, 0.13651108741760254, 0.13179636001586914, 0.13078784942626953, 0.13299918174743652, 0.14012932777404785, 0.126784086227417, 0.13271141052246094, 0.13640928268432617, 0.13241958618164062, 0.1321861743927002, 0.13492751121520996, 0.13139843940734863, 0.13982915878295898, 0.1343076229095459, 0.12694072723388672, 0.13421845436096191, 0.13382911682128906, 0.13566040992736816, 0.13787055015563965, 0.12704014778137207, 0.13964319229125977, 0.13348007202148438, 0.12488722801208496, 0.13689231872558594, 0.13313531875610352, 0.1378035545349121, 0.13575005531311035, 0.12691354751586914, 0.1355586051940918, 0.1330583095550537, 0.13000893592834473, 0.1399843692779541, 0.12733054161071777, 0.13675165176391602, 0.1409602165222168, 0.12541866302490234, 0.13584470748901367, 0.1305079460144043, 0.1309828758239746, 0.13665103912353516, 0.13239574432373047, 0.13547921180725098, 0.13254427909851074, 0.1300654411315918, 0.13890457153320312, 0.13116955757141113, 0.13098406791687012, 0.1378498077392578, 0.13244032859802246, 0.13470029830932617, 0.12992262840270996, 0.13517546653747559, 0.13503766059875488, 0.13050103187561035, 0.1342766284942627, 0.13587498664855957, 0.13313698768615723, 0.1328716278076172, 0.13475298881530762, 0.13165903091430664, 0.13708829879760742, 0.1332416534423828, 0.1316227912902832, 0.13358426094055176, 0.1311495304107666, 0.14054131507873535, 0.12809300422668457, 0.1346895694732666, 0.13017821311950684, 0.13333940505981445, 0.13524770736694336, 0.13180065155029297, 0.13749384880065918, 0.13081908226013184, 0.13098835945129395, 0.13410401344299316, 0.13814425468444824, 0.13006591796875, 0.1321401596069336, 0.1361086368560791, 0.13643956184387207, 0.13391637802124023, 0.12724971771240234, 0.08148980140686035, 0.1868753433227539, 0.13679146766662598, 0.12498331069946289, 0.08923768997192383, 0.17966389656066895, 0.08772087097167969, 0.17938637733459473, 0.130540132522583, 0.08997893333435059, 0.12944793701171875, 0.13526368141174316, 0.13207697868347168, 0.17786765098571777, 0.14151906967163086, 0.140913724899292, 0.1332075595855713, 0.12656641006469727, 0.13508820533752441, 0.13484597206115723, 0.13435077667236328, 0.12901639938354492, 0.14441204071044922, 0.12416887283325195, 0.13086485862731934, 0.13735175132751465, 0.1322486400604248, 0.13427376747131348, 0.13256454467773438, 0.12998700141906738, 0.13859057426452637, 0.12978100776672363, 0.13530921936035156, 0.13777470588684082, 0.12964415550231934, 0.13707351684570312, 0.13360333442687988, 0.1279621124267578, 0.0857698917388916, 0.13149738311767578, 0.13123846054077148, 0.18392062187194824, 0.13336706161499023, 0.0872349739074707, 0.13268184661865234, 0.18006157875061035, 0.08595609664916992, 0.18100595474243164, 0.13073992729187012, 0.14041852951049805, 0.13358306884765625, 0.13327765464782715, 0.13461971282958984, 0.1335914134979248, 0.13655471801757812, 0.13026928901672363, 0.13258075714111328, 0.14020419120788574, 0.08271479606628418, 0.1296389102935791, 0.1289513111114502, 0.1823139190673828, 0.13808965682983398, 0.13090205192565918, 0.08181166648864746, 0.13471579551696777, 0.18189668655395508, 0.13836407661437988, 0.13387250900268555, 0.13216519355773926, 0.13408374786376953, 0.13310623168945312, 0.13616681098937988, 0.1323530673980713, 0.1340477466583252, 0.13541460037231445, 0.13069534301757812, 0.13528966903686523, 0.1357419490814209, 0.13391757011413574, 0.1260051727294922, 0.1363849639892578, 0.13349199295043945, 0.13474273681640625, 0.1321547031402588, 0.1314535140991211, 0.13529181480407715, 0.135345458984375, 0.12941455841064453, 0.1355438232421875, 0.13454627990722656, 0.13657331466674805, 0.12984442710876465, 0.12976574897766113, 0.13845062255859375, 0.13472938537597656, 0.13321471214294434, 0.13197708129882812, 0.1377573013305664, 0.12854695320129395, 0.13488984107971191, 0.13200736045837402, 0.1365184783935547, 0.13020062446594238, 0.13488459587097168, 0.13601016998291016, 0.13293933868408203, 0.13161897659301758, 0.13348007202148438, 0.12983393669128418, 0.13674473762512207, 0.13727998733520508, 0.08193826675415039, 0.12917566299438477, 0.131333589553833, 0.13687443733215332, 0.12952136993408203, 0.1788475513458252, 0.13800644874572754, 0.08719229698181152, 0.13260269165039062, 0.13241004943847656, 0.17763710021972656, 0.13518309593200684, 0.14061260223388672, 0.13113117218017578, 0.13766121864318848, 0.13295960426330566, 0.13347077369689941, 0.13060402870178223, 0.13408446311950684, 0.13556241989135742, 0.1354210376739502, 0.1292705535888672, 0.1362166404724121, 0.13507366180419922, 0.13530516624450684, 0.1306910514831543, 0.1357250213623047, 0.13463139533996582, 0.12853646278381348, 0.13389945030212402, 0.13386774063110352, 0.13243532180786133, 0.1354970932006836, 0.13367867469787598, 0.1318657398223877, 0.14055466651916504, 0.12427115440368652, 0.13402628898620605, 0.14067411422729492, 0.1277463436126709, 0.13707709312438965, 0.1321730613708496, 0.13490843772888184, 0.13163447380065918, 0.13489103317260742, 0.12829160690307617, 0.13647174835205078, 0.1299149990081787, 0.134873628616333, 0.13117098808288574, 0.13587427139282227, 0.13385987281799316, 0.13331890106201172, 0.13094639778137207, 0.13867568969726562, 0.12955069541931152, 0.13640332221984863, 0.13092398643493652, 0.13192439079284668, 0.13804149627685547, 0.13212156295776367, 0.1304020881652832, 0.08700442314147949, 0.18023109436035156, 0.13550209999084473, 0.1322164535522461, 0.13251829147338867, 0.13852524757385254, 0.13182830810546875, 0.13045215606689453, 0.13570380210876465, 0.13634562492370605, 0.13320660591125488, 0.1301729679107666, 0.1338815689086914, 0.13445568084716797, 0.12995362281799316, 0.13610076904296875]}}
--------------
{'vitpose-s-wholebody': {'Average inference fps': 7.496722280892067,
                         'Average inference time': 0.13339162937232427,
                         'inference count': 445}}
