apex is not installed
apex is not installed
apex is not installed
load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/ssd/ssdlite_mobilenetv2_scratch_600e_coco/ssdlite_mobilenetv2_scratch_600e_coco_20210629_110627-974d9307.pth
load checkpoint from local path: pytorch-checkpoint-models/vitpose+_small_whole.pth
The model and loaded state dict do not match exactly

size mismatch for backbone.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for backbone.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([288, 1536]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
size mismatch for backbone.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([288]) from checkpoint, the shape in current model is torch.Size([384]).
size mismatch for keypoint_head.final_layer.weight: copying a param with shape torch.Size([17, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([133, 256, 1, 1]).
size mismatch for keypoint_head.final_layer.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([133]).
unexpected key in source state_dict: associate_keypoint_heads.0.deconv_layers.0.weight, associate_keypoint_heads.0.deconv_layers.1.weight, associate_keypoint_heads.0.deconv_layers.1.bias, associate_keypoint_heads.0.deconv_layers.1.running_mean, associate_keypoint_heads.0.deconv_layers.1.running_var, associate_keypoint_heads.0.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.0.deconv_layers.3.weight, associate_keypoint_heads.0.deconv_layers.4.weight, associate_keypoint_heads.0.deconv_layers.4.bias, associate_keypoint_heads.0.deconv_layers.4.running_mean, associate_keypoint_heads.0.deconv_layers.4.running_var, associate_keypoint_heads.0.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.0.final_layer.weight, associate_keypoint_heads.0.final_layer.bias, associate_keypoint_heads.1.deconv_layers.0.weight, associate_keypoint_heads.1.deconv_layers.1.weight, associate_keypoint_heads.1.deconv_layers.1.bias, associate_keypoint_heads.1.deconv_layers.1.running_mean, associate_keypoint_heads.1.deconv_layers.1.running_var, associate_keypoint_heads.1.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.1.deconv_layers.3.weight, associate_keypoint_heads.1.deconv_layers.4.weight, associate_keypoint_heads.1.deconv_layers.4.bias, associate_keypoint_heads.1.deconv_layers.4.running_mean, associate_keypoint_heads.1.deconv_layers.4.running_var, associate_keypoint_heads.1.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.1.final_layer.weight, associate_keypoint_heads.1.final_layer.bias, associate_keypoint_heads.2.deconv_layers.0.weight, associate_keypoint_heads.2.deconv_layers.1.weight, associate_keypoint_heads.2.deconv_layers.1.bias, associate_keypoint_heads.2.deconv_layers.1.running_mean, associate_keypoint_heads.2.deconv_layers.1.running_var, associate_keypoint_heads.2.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.2.deconv_layers.3.weight, associate_keypoint_heads.2.deconv_layers.4.weight, associate_keypoint_heads.2.deconv_layers.4.bias, associate_keypoint_heads.2.deconv_layers.4.running_mean, associate_keypoint_heads.2.deconv_layers.4.running_var, associate_keypoint_heads.2.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.2.final_layer.weight, associate_keypoint_heads.2.final_layer.bias, associate_keypoint_heads.3.deconv_layers.0.weight, associate_keypoint_heads.3.deconv_layers.1.weight, associate_keypoint_heads.3.deconv_layers.1.bias, associate_keypoint_heads.3.deconv_layers.1.running_mean, associate_keypoint_heads.3.deconv_layers.1.running_var, associate_keypoint_heads.3.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.3.deconv_layers.3.weight, associate_keypoint_heads.3.deconv_layers.4.weight, associate_keypoint_heads.3.deconv_layers.4.bias, associate_keypoint_heads.3.deconv_layers.4.running_mean, associate_keypoint_heads.3.deconv_layers.4.running_var, associate_keypoint_heads.3.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.3.final_layer.weight, associate_keypoint_heads.3.final_layer.bias, associate_keypoint_heads.4.deconv_layers.0.weight, associate_keypoint_heads.4.deconv_layers.1.weight, associate_keypoint_heads.4.deconv_layers.1.bias, associate_keypoint_heads.4.deconv_layers.1.running_mean, associate_keypoint_heads.4.deconv_layers.1.running_var, associate_keypoint_heads.4.deconv_layers.1.num_batches_tracked, associate_keypoint_heads.4.deconv_layers.3.weight, associate_keypoint_heads.4.deconv_layers.4.weight, associate_keypoint_heads.4.deconv_layers.4.bias, associate_keypoint_heads.4.deconv_layers.4.running_mean, associate_keypoint_heads.4.deconv_layers.4.running_var, associate_keypoint_heads.4.deconv_layers.4.num_batches_tracked, associate_keypoint_heads.4.final_layer.weight, associate_keypoint_heads.4.final_layer.bias, backbone.blocks.0.mlp.experts.0.weight, backbone.blocks.0.mlp.experts.0.bias, backbone.blocks.0.mlp.experts.1.weight, backbone.blocks.0.mlp.experts.1.bias, backbone.blocks.0.mlp.experts.2.weight, backbone.blocks.0.mlp.experts.2.bias, backbone.blocks.0.mlp.experts.3.weight, backbone.blocks.0.mlp.experts.3.bias, backbone.blocks.0.mlp.experts.4.weight, backbone.blocks.0.mlp.experts.4.bias, backbone.blocks.0.mlp.experts.5.weight, backbone.blocks.0.mlp.experts.5.bias, backbone.blocks.1.mlp.experts.0.weight, backbone.blocks.1.mlp.experts.0.bias, backbone.blocks.1.mlp.experts.1.weight, backbone.blocks.1.mlp.experts.1.bias, backbone.blocks.1.mlp.experts.2.weight, backbone.blocks.1.mlp.experts.2.bias, backbone.blocks.1.mlp.experts.3.weight, backbone.blocks.1.mlp.experts.3.bias, backbone.blocks.1.mlp.experts.4.weight, backbone.blocks.1.mlp.experts.4.bias, backbone.blocks.1.mlp.experts.5.weight, backbone.blocks.1.mlp.experts.5.bias, backbone.blocks.2.mlp.experts.0.weight, backbone.blocks.2.mlp.experts.0.bias, backbone.blocks.2.mlp.experts.1.weight, backbone.blocks.2.mlp.experts.1.bias, backbone.blocks.2.mlp.experts.2.weight, backbone.blocks.2.mlp.experts.2.bias, backbone.blocks.2.mlp.experts.3.weight, backbone.blocks.2.mlp.experts.3.bias, backbone.blocks.2.mlp.experts.4.weight, backbone.blocks.2.mlp.experts.4.bias, backbone.blocks.2.mlp.experts.5.weight, backbone.blocks.2.mlp.experts.5.bias, backbone.blocks.3.mlp.experts.0.weight, backbone.blocks.3.mlp.experts.0.bias, backbone.blocks.3.mlp.experts.1.weight, backbone.blocks.3.mlp.experts.1.bias, backbone.blocks.3.mlp.experts.2.weight, backbone.blocks.3.mlp.experts.2.bias, backbone.blocks.3.mlp.experts.3.weight, backbone.blocks.3.mlp.experts.3.bias, backbone.blocks.3.mlp.experts.4.weight, backbone.blocks.3.mlp.experts.4.bias, backbone.blocks.3.mlp.experts.5.weight, backbone.blocks.3.mlp.experts.5.bias, backbone.blocks.4.mlp.experts.0.weight, backbone.blocks.4.mlp.experts.0.bias, backbone.blocks.4.mlp.experts.1.weight, backbone.blocks.4.mlp.experts.1.bias, backbone.blocks.4.mlp.experts.2.weight, backbone.blocks.4.mlp.experts.2.bias, backbone.blocks.4.mlp.experts.3.weight, backbone.blocks.4.mlp.experts.3.bias, backbone.blocks.4.mlp.experts.4.weight, backbone.blocks.4.mlp.experts.4.bias, backbone.blocks.4.mlp.experts.5.weight, backbone.blocks.4.mlp.experts.5.bias, backbone.blocks.5.mlp.experts.0.weight, backbone.blocks.5.mlp.experts.0.bias, backbone.blocks.5.mlp.experts.1.weight, backbone.blocks.5.mlp.experts.1.bias, backbone.blocks.5.mlp.experts.2.weight, backbone.blocks.5.mlp.experts.2.bias, backbone.blocks.5.mlp.experts.3.weight, backbone.blocks.5.mlp.experts.3.bias, backbone.blocks.5.mlp.experts.4.weight, backbone.blocks.5.mlp.experts.4.bias, backbone.blocks.5.mlp.experts.5.weight, backbone.blocks.5.mlp.experts.5.bias, backbone.blocks.6.mlp.experts.0.weight, backbone.blocks.6.mlp.experts.0.bias, backbone.blocks.6.mlp.experts.1.weight, backbone.blocks.6.mlp.experts.1.bias, backbone.blocks.6.mlp.experts.2.weight, backbone.blocks.6.mlp.experts.2.bias, backbone.blocks.6.mlp.experts.3.weight, backbone.blocks.6.mlp.experts.3.bias, backbone.blocks.6.mlp.experts.4.weight, backbone.blocks.6.mlp.experts.4.bias, backbone.blocks.6.mlp.experts.5.weight, backbone.blocks.6.mlp.experts.5.bias, backbone.blocks.7.mlp.experts.0.weight, backbone.blocks.7.mlp.experts.0.bias, backbone.blocks.7.mlp.experts.1.weight, backbone.blocks.7.mlp.experts.1.bias, backbone.blocks.7.mlp.experts.2.weight, backbone.blocks.7.mlp.experts.2.bias, backbone.blocks.7.mlp.experts.3.weight, backbone.blocks.7.mlp.experts.3.bias, backbone.blocks.7.mlp.experts.4.weight, backbone.blocks.7.mlp.experts.4.bias, backbone.blocks.7.mlp.experts.5.weight, backbone.blocks.7.mlp.experts.5.bias, backbone.blocks.8.mlp.experts.0.weight, backbone.blocks.8.mlp.experts.0.bias, backbone.blocks.8.mlp.experts.1.weight, backbone.blocks.8.mlp.experts.1.bias, backbone.blocks.8.mlp.experts.2.weight, backbone.blocks.8.mlp.experts.2.bias, backbone.blocks.8.mlp.experts.3.weight, backbone.blocks.8.mlp.experts.3.bias, backbone.blocks.8.mlp.experts.4.weight, backbone.blocks.8.mlp.experts.4.bias, backbone.blocks.8.mlp.experts.5.weight, backbone.blocks.8.mlp.experts.5.bias, backbone.blocks.9.mlp.experts.0.weight, backbone.blocks.9.mlp.experts.0.bias, backbone.blocks.9.mlp.experts.1.weight, backbone.blocks.9.mlp.experts.1.bias, backbone.blocks.9.mlp.experts.2.weight, backbone.blocks.9.mlp.experts.2.bias, backbone.blocks.9.mlp.experts.3.weight, backbone.blocks.9.mlp.experts.3.bias, backbone.blocks.9.mlp.experts.4.weight, backbone.blocks.9.mlp.experts.4.bias, backbone.blocks.9.mlp.experts.5.weight, backbone.blocks.9.mlp.experts.5.bias, backbone.blocks.10.mlp.experts.0.weight, backbone.blocks.10.mlp.experts.0.bias, backbone.blocks.10.mlp.experts.1.weight, backbone.blocks.10.mlp.experts.1.bias, backbone.blocks.10.mlp.experts.2.weight, backbone.blocks.10.mlp.experts.2.bias, backbone.blocks.10.mlp.experts.3.weight, backbone.blocks.10.mlp.experts.3.bias, backbone.blocks.10.mlp.experts.4.weight, backbone.blocks.10.mlp.experts.4.bias, backbone.blocks.10.mlp.experts.5.weight, backbone.blocks.10.mlp.experts.5.bias, backbone.blocks.11.mlp.experts.0.weight, backbone.blocks.11.mlp.experts.0.bias, backbone.blocks.11.mlp.experts.1.weight, backbone.blocks.11.mlp.experts.1.bias, backbone.blocks.11.mlp.experts.2.weight, backbone.blocks.11.mlp.experts.2.bias, backbone.blocks.11.mlp.experts.3.weight, backbone.blocks.11.mlp.experts.3.bias, backbone.blocks.11.mlp.experts.4.weight, backbone.blocks.11.mlp.experts.4.bias, backbone.blocks.11.mlp.experts.5.weight, backbone.blocks.11.mlp.experts.5.bias

Thread "input" started
Thread "det" started
Thread "pose" started
False
Inference data
{'vitpose-s-wholebody': {'inference count': 536, 'inference times': [0.5985085964202881, 0.10405468940734863, 0.07338142395019531, 0.07275199890136719, 0.07307028770446777, 0.0722041130065918, 0.07144284248352051, 0.07244658470153809, 0.07076525688171387, 0.07491850852966309, 0.07505631446838379, 0.0727849006652832, 0.0711357593536377, 0.07343196868896484, 0.07703018188476562, 0.07220578193664551, 0.06929707527160645, 0.07242059707641602, 0.07140350341796875, 0.07308459281921387, 0.07633829116821289, 0.07179546356201172, 0.0700376033782959, 0.07187724113464355, 0.07247352600097656, 0.0816195011138916, 0.07255768775939941, 0.07374453544616699, 0.07295393943786621, 0.07000350952148438, 0.07872414588928223, 0.07237958908081055, 0.07217025756835938, 0.06991720199584961, 0.07239341735839844, 0.070465087890625, 0.0731821060180664, 0.0737302303314209, 0.07355403900146484, 0.07116460800170898, 0.07308840751647949, 0.071319580078125, 0.06861495971679688, 0.07410621643066406, 0.07191801071166992, 0.07115817070007324, 0.07076573371887207, 0.07478761672973633, 0.07189822196960449, 0.07058072090148926, 0.07967424392700195, 0.07342195510864258, 0.06967830657958984, 0.07283377647399902, 0.0712881088256836, 0.07312297821044922, 0.07142806053161621, 0.07142949104309082, 0.07089352607727051, 0.07236433029174805, 0.07285881042480469, 0.07205963134765625, 0.07343339920043945, 0.0701448917388916, 0.07380485534667969, 0.07551455497741699, 1.2372424602508545, 1.3681938648223877, 1.3716812133789062, 1.4158201217651367, 1.4426815509796143, 1.4682142734527588, 1.3545055389404297, 1.3252036571502686, 1.3941447734832764, 1.396033525466919, 1.4130756855010986, 1.5088939666748047, 0.50962233543396, 0.11071181297302246, 0.09356188774108887, 0.09291744232177734, 0.08884644508361816, 0.08930850028991699, 0.08257079124450684, 0.08355045318603516, 0.08921241760253906, 0.09896445274353027, 0.10079336166381836, 0.09989476203918457, 0.09842610359191895, 0.10246443748474121, 0.09378218650817871, 0.09411454200744629, 0.09653949737548828, 0.09518909454345703, 0.09788870811462402, 0.09871077537536621, 0.09594297409057617, 0.09846711158752441, 0.0915842056274414, 0.08966541290283203, 0.09458327293395996, 0.09071803092956543, 0.08328723907470703, 0.0791158676147461, 0.08029818534851074, 0.08027815818786621, 0.09125971794128418, 0.08797740936279297, 0.08608460426330566, 0.08350157737731934, 0.0893256664276123, 0.09896302223205566, 0.08948922157287598, 0.09473276138305664, 0.09418797492980957, 0.09018754959106445, 0.0910196304321289, 0.08918285369873047, 0.09383273124694824, 0.08720803260803223, 0.08636236190795898, 0.08243012428283691, 0.08672666549682617, 0.08262491226196289, 0.08024215698242188, 0.07962894439697266, 0.08080220222473145, 0.08643603324890137, 0.0818331241607666, 0.08090710639953613, 0.08048772811889648, 0.0795292854309082, 0.07835769653320312, 0.0805196762084961, 0.08134937286376953, 0.0819404125213623, 0.08180665969848633, 0.08050990104675293, 0.07971906661987305, 0.07938909530639648, 0.08024787902832031, 0.08466482162475586, 0.08128857612609863, 0.08729100227355957, 0.07862973213195801, 0.07865595817565918, 0.07886600494384766, 0.08035039901733398, 0.08633804321289062, 0.07917499542236328, 0.08089852333068848, 0.07747173309326172, 0.09953141212463379, 0.08570623397827148, 0.08217501640319824, 0.08755183219909668, 0.08514046669006348, 0.0856788158416748, 0.087677001953125, 0.08268284797668457, 0.08364176750183105, 0.0806434154510498, 0.08277273178100586, 0.08420228958129883, 0.07992887496948242, 0.08701372146606445, 0.08831000328063965, 0.07962226867675781, 0.08543133735656738, 0.08440685272216797, 0.08742547035217285, 0.08191704750061035, 0.09010744094848633, 0.08470654487609863, 0.0821218490600586, 0.08135628700256348, 0.08932137489318848, 0.09365391731262207, 0.09365415573120117, 0.09234070777893066, 0.09042930603027344, 0.09511017799377441, 0.08996748924255371, 0.08946108818054199, 0.09615206718444824, 0.08276820182800293, 0.0833587646484375, 0.08090400695800781, 0.08444643020629883, 0.08645892143249512, 0.08370637893676758, 0.08024382591247559, 0.0779428482055664, 0.0805201530456543, 0.08250880241394043, 0.08151984214782715, 0.08456277847290039, 0.07787609100341797, 0.08090829849243164, 0.08104348182678223, 0.08044981956481934, 0.07841730117797852, 0.0823369026184082, 0.07814431190490723, 0.0818183422088623, 0.07873344421386719, 0.07780241966247559, 0.08030509948730469, 0.07725238800048828, 0.07884573936462402, 0.07970356941223145, 0.07822251319885254, 0.07788205146789551, 0.07993745803833008, 0.07702827453613281, 0.08097648620605469, 0.08188176155090332, 0.08048081398010254, 0.07949447631835938, 0.07963895797729492, 0.07834601402282715, 0.07854580879211426, 0.08291864395141602, 0.08116674423217773, 0.0790402889251709, 0.07829928398132324, 0.08356451988220215, 0.08108925819396973, 0.08472061157226562, 0.07984399795532227, 0.08164262771606445, 0.08339643478393555, 0.07719969749450684, 0.08014750480651855, 0.08094453811645508, 0.0818026065826416, 0.07905149459838867, 0.0824427604675293, 0.07919907569885254, 0.07977771759033203, 0.0812845230102539, 0.07883501052856445, 0.07874298095703125, 0.0797891616821289, 0.08026862144470215, 0.08076643943786621, 0.08053469657897949, 0.07899904251098633, 0.08139824867248535, 0.07946014404296875, 0.07727169990539551, 0.07672786712646484, 0.07743310928344727, 0.08085846900939941, 0.07987356185913086, 0.08158254623413086, 0.07709622383117676, 0.07735991477966309, 0.07982563972473145, 0.07979249954223633, 0.08137392997741699, 0.07897424697875977, 0.08034896850585938, 0.07681012153625488, 0.0805063247680664, 0.08163237571716309, 0.07976818084716797, 0.07898402214050293, 0.08268904685974121, 0.07987523078918457, 0.07773542404174805, 0.08085012435913086, 0.08012628555297852, 0.07903027534484863, 0.07966876029968262, 0.08064079284667969, 0.08041977882385254, 0.08024454116821289, 0.0803370475769043, 0.0789947509765625, 0.08248257637023926, 0.07759976387023926, 0.07882261276245117, 0.0778038501739502, 0.08112955093383789, 0.07818746566772461, 0.07793807983398438, 0.08004450798034668, 0.07729840278625488, 0.08143281936645508, 0.08541321754455566, 0.08115363121032715, 0.07932829856872559, 0.08086776733398438, 0.08228135108947754, 0.08083176612854004, 0.08049726486206055, 0.0800321102142334, 0.07821416854858398, 0.07789993286132812, 0.07893490791320801, 0.07839298248291016, 0.08256101608276367, 0.08126235008239746, 0.08033180236816406, 0.07819151878356934, 0.07926440238952637, 0.07928037643432617, 0.07906818389892578, 0.08043909072875977, 0.07721185684204102, 0.07981085777282715, 0.08014392852783203, 0.07962417602539062, 0.08120441436767578, 0.07820630073547363, 0.08060812950134277, 0.08055233955383301, 0.08016204833984375, 0.07754874229431152, 0.08321499824523926, 0.08165788650512695, 0.07973885536193848, 0.08372640609741211, 0.08091878890991211, 0.07863354682922363, 0.07774019241333008, 0.07934021949768066, 0.08050394058227539, 0.07790422439575195, 0.07977032661437988, 0.08130073547363281, 0.07981038093566895, 0.0791168212890625, 0.08176755905151367, 0.0794377326965332, 0.07879185676574707, 0.08549189567565918, 0.08174729347229004, 0.08634066581726074, 0.08393359184265137, 0.0799856185913086, 0.08150386810302734, 0.0806128978729248, 0.08366537094116211, 0.07921099662780762, 0.07871127128601074, 0.07926464080810547, 0.08117079734802246, 0.0799856185913086, 0.07623505592346191, 0.07875561714172363, 0.0811309814453125, 0.08120179176330566, 0.08005309104919434, 0.08787178993225098, 0.08211493492126465, 0.08046221733093262, 0.0838468074798584, 0.08212780952453613, 0.07980132102966309, 0.08082294464111328, 0.07911849021911621, 0.08078837394714355, 0.07826614379882812, 0.08062481880187988, 0.08296012878417969, 0.08297562599182129, 0.07832884788513184, 0.08182406425476074, 0.08188986778259277, 0.08081936836242676, 0.08136367797851562, 0.0818178653717041, 0.07986569404602051, 0.08029699325561523, 0.08024191856384277, 0.0835270881652832, 0.07829046249389648, 0.08047199249267578, 0.08065986633300781, 0.07921934127807617, 0.08479070663452148, 0.08041238784790039, 0.07751655578613281, 0.07915449142456055, 0.08485937118530273, 0.0809640884399414, 0.07877755165100098, 0.08179259300231934, 0.07795834541320801, 0.07834863662719727, 0.0785362720489502, 0.07966089248657227, 0.07864737510681152, 0.07966470718383789, 0.08086395263671875, 0.07955360412597656, 0.07846784591674805, 0.0805349349975586, 0.08000373840332031, 0.08034062385559082, 0.08151841163635254, 0.08599090576171875, 0.07993102073669434, 0.08030486106872559, 0.07941317558288574, 0.08156037330627441, 0.07904982566833496, 0.07865309715270996, 0.07943534851074219, 0.07948946952819824, 0.08503293991088867, 0.07920074462890625, 0.07757782936096191, 0.07993412017822266, 0.0801398754119873, 0.08195829391479492, 0.0798187255859375, 0.07918906211853027, 0.07855486869812012, 0.07933974266052246, 0.0777430534362793, 0.08403730392456055, 0.07909274101257324, 0.08555817604064941, 0.07880187034606934, 0.08140826225280762, 0.08406829833984375, 0.08144044876098633, 0.0841825008392334, 0.08249592781066895, 0.07819247245788574, 0.07849836349487305, 0.07941389083862305, 0.08312368392944336, 0.08098578453063965, 0.08267712593078613, 0.08150649070739746, 0.07830190658569336, 0.07769632339477539, 0.07838034629821777, 0.07950425148010254, 0.08023977279663086, 0.08331441879272461, 0.08176445960998535, 0.07963347434997559, 0.08123588562011719, 0.07880568504333496, 0.07995724678039551, 0.08435273170471191, 0.08246660232543945, 0.0851588249206543, 0.08011651039123535, 0.0794517993927002, 0.07834196090698242, 0.07966804504394531, 0.07846832275390625, 0.07958197593688965, 0.08120083808898926, 0.0795755386352539, 0.0801079273223877, 0.08209538459777832, 0.07910561561584473, 0.08009052276611328, 0.07978630065917969, 0.08607769012451172, 0.07967066764831543, 0.07910847663879395, 0.0784454345703125, 0.08018064498901367, 0.0795443058013916, 0.0788416862487793, 0.08078360557556152, 0.08136153221130371, 0.08875823020935059, 0.07862067222595215, 0.07946991920471191, 0.08102273941040039, 0.08020424842834473, 0.08226251602172852, 0.08054423332214355, 0.0799093246459961, 0.08219552040100098, 0.08245468139648438, 0.07883739471435547, 0.08266282081604004, 0.07977890968322754, 0.07906746864318848, 0.08263468742370605, 0.08240222930908203, 0.07812237739562988, 0.07779455184936523, 0.08348417282104492, 0.08153820037841797, 0.07791399955749512, 0.09290027618408203, 0.08044695854187012, 0.0803072452545166, 0.08191466331481934, 0.08064723014831543, 0.08103466033935547, 0.07993316650390625, 0.07984781265258789, 0.08034420013427734, 0.07856512069702148, 0.08017730712890625, 0.08036994934082031, 0.07784080505371094, 0.08032655715942383, 0.08063387870788574, 0.07922697067260742, 0.0789799690246582, 0.07891440391540527, 0.08360981941223145, 0.08186125755310059, 0.07736063003540039, 0.08173871040344238, 0.08048868179321289, 0.08004236221313477, 0.08069038391113281, 0.08055663108825684, 0.07919502258300781, 0.07690143585205078, 0.07917046546936035, 0.07933974266052246, 0.08061909675598145, 0.07859945297241211, 0.07939291000366211, 0.08074092864990234, 0.08240127563476562, 0.0804131031036377, 0.07906270027160645, 0.08477950096130371]}}
--------------
{'vitpose-s-wholebody': {'Average inference fps': 12.45939055435424,
                         'Average inference time': 0.08026074755723309,
                         'inference count': 536}}
